{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_iRFpa5r6Ry",
        "outputId": "e1e28a01-cdfe-478a-df43-48fc37296ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paper Preprocessing"
      ],
      "metadata": {
        "id": "7pwPWbZ0BPFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect the data\n",
        "data = pd.read_csv('Data Set 1.csv')\n",
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2V6aTeB6bC",
        "outputId": "c66bce04-cd31-4b16-eaa9-80af23310ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['meanA', 'meanAstd', 'meangstd', 'stdmeanA', 'stdstda', 'meang',\n",
            "       'skewmeanA', 'skewstdA', 'skewG', 'minmeanA', 'minstdA', 'minG',\n",
            "       'maxmeanA', 'maxstdA', 'maxmG', 'kurtmeanA', 'kurtstdA', 'kurtG',\n",
            "       'entromeanA', 'entrostdA', 'entroG', 'iqrmeanA', 'iqrstdA', 'iqrG',\n",
            "       'class'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('Data Set 1.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "features = data.drop(columns=['class'])\n",
        "labels = data['class']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE()\n",
        "features_resampled, labels_resampled = smote.fit_resample(features_scaled, labels)\n",
        "\n",
        "# Save the oversampled dataset\n",
        "oversampled_data = pd.DataFrame(features_resampled, columns=features.columns)\n",
        "oversampled_data['class'] = labels_resampled\n",
        "oversampled_data.to_csv('Data Set 1 OverSampled.csv', index=False)\n",
        "\n",
        "print(\"Data preprocessing and oversampling completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMGHQzHBO1t",
        "outputId": "20270cca-8589-4799-b318-1b8430c9ec83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing and oversampling completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYzlbsm5ZN-k"
      },
      "source": [
        "## 1 CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV6U2kqDZNLV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 3: Load the data from Google Drive\n",
        "# Make sure to place \"Data Set 1 OverSampled.csv\" in a known directory in your Google Drive\n",
        "data_path = 'Data Set 1 OverSampled.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Step 4: Preprocess the data\n",
        "X = df.drop('class', axis=1)  # Assuming 'class' is the column name for labels\n",
        "y = df['class']\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Cross-validation and Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 7: Evaluate the best model using cross-validation\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
        "print(f'Cross-validation accuracy: {cv_scores.mean()}')\n",
        "\n",
        "# Step 8: Test the model on the testing set\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy on the test data: {test_accuracy}')\n",
        "\n",
        "# Step 9: Save the best model to Google Drive\n",
        "model_path = '/content/drive/My Drive/anomaly_detection_model.joblib'\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f'Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN5Pz0BlZQoO"
      },
      "source": [
        "##2 CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzEGqjTdVyQp",
        "outputId": "55fed472-e9c9-4292-b1fb-f70abb4e04e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8861788617886179\n",
            "Model saved to /content/drive/My Drive/anomaly_detection_model.joblib\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 3: Load the data from Google Drive\n",
        "# Make sure to place \"Data Set 1 OverSampled.csv\" and \"Data Set 2.csv\" in known directories in your Google Drive\n",
        "data_path1 = 'Data Set 1 OverSampled.csv'\n",
        "data_path2 = 'Data Set 2 OverSampled.csv'\n",
        "\n",
        "df1 = pd.read_csv(data_path1)\n",
        "df2 = pd.read_csv(data_path2)\n",
        "\n",
        "# Step 4: Combine the data\n",
        "df_combined = pd.concat([df1, df2])\n",
        "\n",
        "# Step 5: Preprocess the data\n",
        "X = df_combined.drop('class', axis=1)  # Assuming 'class' is the column name for labels\n",
        "y = df_combined['class']\n",
        "\n",
        "# Step 6: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Step 9: Save the model to Google Drive\n",
        "model_path = '/content/drive/My Drive/anomaly_detection_model.joblib'\n",
        "joblib.dump(model, model_path)\n",
        "print(f'Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJJtXw-ZTit"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wP5gEHFWcbY",
        "outputId": "12479719-23b1-4a5e-904d-b482a2045ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test data: 0.9755952380952381\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 3: Load the saved model from Google Drive\n",
        "model_path = '/content/drive/My Drive/anomaly_detection_model.joblib'\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# Step 4: Load the new test data from Google Drive\n",
        "# Make sure to place \"New_Test_Data.csv\" in a known directory in your Google Drive\n",
        "test_data_path = 'Data Set 1 OverSampled.csv'\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Step 5: Preprocess the test data\n",
        "# Assuming 'class' is the column name for labels in the test data, if not available, remove the next line\n",
        "X_test = test_df.drop('class', axis=1)  # Features\n",
        "y_test = test_df['class']  # Actual labels\n",
        "\n",
        "# Step 6: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate predictions (if actual labels are available)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy on the test data: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv3WXWCZbqsR"
      },
      "source": [
        "##Paper way to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3F5OfCbveo",
        "outputId": "1d3b3579-6ebb-4377-dd24-e728fb7b3874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:814: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  results[i] = self.f(v)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.95, 0.15, '/content/drive/MyDrive/New_Dataset.pkl')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Data Set 1 OverSampled.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function to extract features and ensure numeric values\n",
        "def extract_features(df):\n",
        "    features = {}\n",
        "    features['mean'] = df.mean().mean()\n",
        "    features['std'] = df.std().mean()\n",
        "    features['skewness'] = df.apply(skew).mean()\n",
        "    features['kurtosis'] = df.apply(kurtosis).mean()\n",
        "    features['max'] = df.max().max()\n",
        "    features['min'] = df.min().min()\n",
        "    return features\n",
        "\n",
        "# Apply sliding window\n",
        "window_size = 100  # assuming 100 Hz sample rate, 1 second window\n",
        "step_size = window_size // 2  # 50% overlap\n",
        "\n",
        "feature_list = []\n",
        "\n",
        "for start in range(0, len(data) - window_size + 1, step_size):\n",
        "    window = data.iloc[start:start + window_size]\n",
        "    features = extract_features(window)\n",
        "    feature_list.append(features)\n",
        "\n",
        "# Convert list of feature dicts to DataFrame\n",
        "features_df = pd.DataFrame(feature_list)\n",
        "\n",
        "# Extract target variable\n",
        "target = data['class'][window_size//2::step_size][:len(features_df)]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_df, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest model\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
        "\n",
        "# Save the trained model\n",
        "model_path = '/content/drive/MyDrive/New_Dataset.pkl'\n",
        "joblib.dump(clf, model_path)\n",
        "\n",
        "# Output results\n",
        "cv_scores.mean(), cv_scores.std(), model_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Data Set 1 OverSampled.csv')\n",
        "\n",
        "# Inspect the dataset to confirm the column names\n",
        "print(data.head())\n",
        "print(data.columns)\n",
        "\n",
        "# Assuming these are the correct columns for accelerometer and gyroscope statistics\n",
        "# The column names are adapted based on the provided column names in your dataset\n",
        "feature_columns = [\n",
        "    'meanA', 'meanAstd', 'meangstd', 'stdmeanA', 'stdstda', 'meang',\n",
        "    'skewmeanA', 'skewstdA', 'skewG', 'minmeanA', 'minstdA', 'minG',\n",
        "    'maxmeanA', 'maxstdA', 'maxmG', 'kurtmeanA', 'kurtstdA', 'kurtG',\n",
        "    'entromeanA', 'entrostdA', 'entroG', 'iqrmeanA', 'iqrstdA', 'iqrG'\n",
        "]\n",
        "\n",
        "# Prepare the feature matrix and labels\n",
        "X = data[feature_columns]\n",
        "y = data['class']  # Assuming 'class' is the column with labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'anomaly_detection_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIpjAMU1pAs7",
        "outputId": "e8c81116-5d2b-4d16-d32b-1fbce18e9c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      meanA  meanAstd  meangstd  stdmeanA   stdstda     meang  skewmeanA  \\\n",
            "0  1.017204  0.058578  0.642445  0.053185  0.048299  0.526831   0.934144   \n",
            "1  1.003853  0.005816  0.168383  0.003261  0.002917  0.207648  -0.410724   \n",
            "2  1.003775  0.023734  0.368631  0.025993  0.025151  0.368631   1.852045   \n",
            "3  1.004330  0.016759  0.279067  0.014712  0.009971  0.255733   0.474464   \n",
            "4  1.004302  0.009317  0.271160  0.002629  0.003489  0.233640   0.456424   \n",
            "\n",
            "   skewstdA     skewG  minmeanA  ...  kurtmeanA  kurtstdA     kurtG  \\\n",
            "0  1.157455  1.658387  0.928243  ...   0.579301  0.791138  3.657740   \n",
            "1  0.751071  0.904551  0.990956  ...   0.459438 -0.048304  0.222102   \n",
            "2  2.815163  0.596992  0.967968  ...   4.331743  8.655530 -0.432390   \n",
            "3  0.757765  0.330055  0.984220  ...   0.851071  0.266029 -0.800815   \n",
            "4  0.464779  0.607188  0.999489  ...  -0.543562  0.111179 -0.785866   \n",
            "\n",
            "   entromeanA  entrostdA    entroG  iqrmeanA   iqrstdA      iqrG  class  \n",
            "0    4.604300   4.301860  4.511478  0.033730  0.054228  0.170014      1  \n",
            "1    4.605161   4.467455  4.420494  0.005208  0.004814  0.117449      1  \n",
            "2    4.604840   4.203549  4.460488  0.029840  0.018948  0.274239      1  \n",
            "3    4.605063   4.519563  4.525149  0.021538  0.031835  0.123389      1  \n",
            "4    4.605168   4.543790  4.370444  0.003429  0.003928  0.331182      1  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "Index(['meanA', 'meanAstd', 'meangstd', 'stdmeanA', 'stdstda', 'meang',\n",
            "       'skewmeanA', 'skewstdA', 'skewG', 'minmeanA', 'minstdA', 'minG',\n",
            "       'maxmeanA', 'maxstdA', 'maxmG', 'kurtmeanA', 'kurtstdA', 'kurtG',\n",
            "       'entromeanA', 'entrostdA', 'entroG', 'iqrmeanA', 'iqrstdA', 'iqrG',\n",
            "       'class'],\n",
            "      dtype='object')\n",
            "Accuracy: 0.9087301587301587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.86      0.91       258\n",
            "           1       0.87      0.96      0.91       246\n",
            "\n",
            "    accuracy                           0.91       504\n",
            "   macro avg       0.91      0.91      0.91       504\n",
            "weighted avg       0.91      0.91      0.91       504\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjk7sNpndVlL"
      },
      "source": [
        "##Test Research paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAP6Iog0dVMI",
        "outputId": "5cf1ad60-76c5-4841-a069-ad746f794c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:814: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  results[i] = self.f(v)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9230769230769231, 0.9226190476190476)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import joblib\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/New_Dataset.pkl'\n",
        "clf = joblib.load(model_path)\n",
        "\n",
        "# Load the new dataset\n",
        "new_dataset_path = '/content/Data Set 2 OverSampled.csv'  # Replace with the path to your new dataset\n",
        "new_data = pd.read_csv(new_dataset_path)\n",
        "\n",
        "# Function to extract features and ensure numeric values\n",
        "def extract_features(df):\n",
        "    features = {}\n",
        "    features['mean'] = df.mean().mean()\n",
        "    features['std'] = df.std().mean()\n",
        "    features['skewness'] = df.apply(skew).mean()\n",
        "    features['kurtosis'] = df.apply(kurtosis).mean()\n",
        "    features['max'] = df.max().max()\n",
        "    features['min'] = df.min().min()\n",
        "    return features\n",
        "\n",
        "# Apply sliding window\n",
        "window_size = 100  # assuming 100 Hz sample rate, 1 second window\n",
        "step_size = window_size // 2  # 50% overlap\n",
        "\n",
        "feature_list = []\n",
        "\n",
        "for start in range(0, len(new_data) - window_size + 1, step_size):\n",
        "    window = new_data.iloc[start:start + window_size]\n",
        "    features = extract_features(window)\n",
        "    feature_list.append(features)\n",
        "\n",
        "# Convert list of feature dicts to DataFrame\n",
        "new_features_df = pd.DataFrame(feature_list)\n",
        "\n",
        "# Extract target variable\n",
        "new_target = new_data['class'][window_size//2::step_size][:len(new_features_df)]\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = clf.predict(new_features_df)\n",
        "\n",
        "# Evaluate the model's performance on the new dataset\n",
        "accuracy = accuracy_score(new_target, predictions)\n",
        "f1 = f1_score(new_target, predictions, average='weighted')\n",
        "\n",
        "# Output results\n",
        "accuracy, f1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}